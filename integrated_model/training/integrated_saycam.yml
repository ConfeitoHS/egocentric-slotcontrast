# Integrated Training Configuration
# Combines: Silicon-Menagerie ViT + STEVE Slot Attention + SlotContrast Temporal Consistency

experiment_group: integrated_model
experiment_name: silicon_steve_slotcontrast

globals:
  # Slot configuration (from STEVE)
  NUM_SLOTS: 7  # Number of object slots
  SLOT_DIM: 128  # Slot representation dimension

  # Silicon-Menagerie ViT configuration
  SAYCAM_MODEL: dino_say_vitb14  # Pretrained model name
  FEAT_DIM: 768  # ViT-B/14 feature dimension
  NUM_PATCHES: 256  # 16x16 patches for 224x224 image

  # Training configuration
  NUM_GPUS: 1
  BATCH_SIZE_PER_GPU: 16  # Adjust based on GPU memory
  TOTAL_BATCH_SIZE: "${mul: ${.NUM_GPUS}, ${.BATCH_SIZE_PER_GPU}}"

  # Learning rates
  BASE_LR: 0.0001
  SLOT_LR: 0.0004  # Higher LR for slot attention (trainable)
  BACKBONE_LR: 0.00001  # Lower LR if backbone is trainable

trainer:
  max_steps: 100000
  log_every_n_steps: 500
  val_check_interval: 2500
  gradient_clip_val: 0.05
  accelerator: auto
  devices: ${globals.NUM_GPUS}

checkpoint_every_n_steps: 2500

optimizer:
  name: Adam
  lr: ${globals.SLOT_LR}
  lr_scheduler:
    name: exp_decay_with_warmup
    warmup_steps: 2500
    decay_steps: ${trainer.max_steps}

model:
  input_type: video  # Process videos, not single images
  visualize: true
  visualize_every_n_steps: 5000

  # Loss configuration
  losses:
    # Feature reconstruction loss (STEVE-style)
    loss_featrec:
      name: MSELoss
      pred_dims:
        - 0
        - ${globals.FEAT_DIM}
      pred_key: decoder.reconstruction
      target_key: encoder.backbone_features

    # Slot-slot contrastive loss (SlotContrast)
    loss_ss:
      name: Slot_Slot_Contrastive_Loss
      pred_key: processor.state  # Slots from processor
      temperature: 0.1
      batch_contrast: true
      patch_inputs: false
      keep_input_dim: true

  loss_weights:
    loss_featrec: 1.0  # Feature reconstruction weight
    loss_ss: 0.5  # Temporal consistency weight

  # Slot initializer (learned initialization)
  initializer:
    name: FixedLearnedInit
    n_slots: ${globals.NUM_SLOTS}
    dim: ${globals.SLOT_DIM}

  # Encoder: Silicon-Menagerie pretrained ViT
  encoder:
    # Note: This will use custom encoder from custom_encoders.py
    # You need to register it in the SlotContrast framework
    backbone:
      name: TimmExtractor  # Using SlotContrast's TimmExtractor for now
      model: ${globals.SAYCAM_MODEL}
      features:
        - vit_block12  # Extract features from last ViT block
      frozen: true  # Keep pretrained backbone frozen
      pretrained: true
      model_kwargs:
        dynamic_img_size: true

    # Transform ViT features to slot dimension
    output_transform:
      name: networks.two_layer_mlp
      inp_dim: ${globals.FEAT_DIM}
      outp_dim: ${globals.SLOT_DIM}
      hidden_dim: "${mul: ${globals.FEAT_DIM}, 2}"
      layer_norm: true

  # Grouper: Slot Attention (STEVE-style)
  grouper:
    name: SlotAttention
    inp_dim: ${globals.SLOT_DIM}
    slot_dim: ${globals.SLOT_DIM}
    n_iters: 3  # Number of slot attention iterations
    use_mlp: true  # Use MLP after GRU update
    use_gru: true  # Use GRU for slot update

  # Latent processor for temporal consistency
  latent_processor:
    first_step_corrector_args:
      n_iters: 3  # Slot attention iterations for first frame

  # Decoder: Reconstruct ViT features from slots
  decoder:
    name: MLPDecoder
    inp_dim: ${globals.SLOT_DIM}
    outp_dim: ${globals.FEAT_DIM}
    hidden_dims: [1024, 1024, 1024]  # 3-layer MLP
    n_patches: ${globals.NUM_PATCHES}

  # Predictor: Temporal dynamics (optional, for future frames)
  predictor:
    name: networks.TransformerEncoder
    dim: ${globals.SLOT_DIM}
    n_blocks: 2  # 2 transformer blocks
    n_heads: 4

# Optional: Validation metrics
val_metrics:
  # Uncomment if you have segmentation ground truth
  # ari:
  #   name: VideoARI
  #   ignore_background: true
  #   pred_key: decoder_masks_hard
  #   true_key: segmentations
  # mbo:
  #   name: VideoIoU
  #   matching: overlap
  #   ignore_background: true
  #   pred_key: decoder_masks_hard
  #   true_key: segmentations

# Dataset configuration
dataset:
  name: SAYCAMDataModule  # Use your SAYCam datamodule

  # Update these paths to your data
  json_dir: "/path/to/saycam/json"  # Path to SAYCam metadata
  img_dir: "/path/to/saycam/frames"  # Path to SAYCam frames

  img_size: 224  # Image size for ViT
  ep_length: 8  # Number of frames per video clip
  batch_size: ${globals.BATCH_SIZE_PER_GPU}
  num_workers: 4  # DataLoader workers

  # Optional data augmentation
  augmentation:
    random_crop: true
    random_horizontal_flip: true
    color_jitter: true
